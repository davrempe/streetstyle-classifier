{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "get_ipython().magic('reload_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streetstyle_dataset import StreetStyleDataset\n",
    "import data_utils\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        data_utils.ResizeTransform(299),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "dataset = StreetStyleDataset('../data/streetstyle27k', '../data/', transform=transform)\n",
    "\n",
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    plt.figure()\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "\n",
    "images, labels = dataset.next_train()\n",
    "img = torchvision.utils.make_grid(images[:4], nrow=2)\n",
    "imshow(img)\n",
    "\n",
    "plt.figure()\n",
    "images, labels = dataset.next_eval()\n",
    "img = torchvision.utils.make_grid(images[:4], nrow=2)\n",
    "imshow(img)\n",
    "\n",
    "images, labels = dataset.next_test()\n",
    "img = torchvision.utils.make_grid(images[:4], nrow=2)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier_model import StreetStyleClassifier\n",
    "from torch.autograd import Variable\n",
    "\n",
    "imag_var = Variable(images)\n",
    "model = StreetStyleClassifier()\n",
    "model.set_eval_attributes([True, True, False, True, False, False,False,False,False,False,False,False])\n",
    "out = model(imag_var)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use our trained best model to infer the unknow images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "get_ipython().magic('reload_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "from newsanchor_classifier_infer import NewsAnchorClassifierInfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = NewsAnchorClassifierInfer(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.create_data_loaders('../../data/cloth/cloth_all_img', '../../data/cloth/cloth_manifest.pkl', batch_size=64)\n",
    "# test.create_data_loaders('../data/cloth_test_img', '../data/cloth_test_manifest.pkl', batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.visualize_single_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parameters(model):\n",
    "    total_num=0\n",
    "    if type(model) == type(dict()):\n",
    "        for key in model:\n",
    "            for p in model[key].parameters():\n",
    "                total_num+=p.nelement()\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            total_num+=p.nelement()\n",
    "    return total_num\n",
    "\n",
    "test.load_model(\"trained_models/newsanchor_850iters_loss_1-3lr_earlystopping.tar\")\n",
    "print(test.model)\n",
    "print('num params: ' + str(get_num_parameters(test.model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_result, feature_result = test.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(infer_result, open('../../data/cloth/cloth_all_infer.pkl', 'wb'), protocol=2)\n",
    "pickle.dump(feature_result, open('../../data/cloth/cloth_all_feature.pkl', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "import cv2\n",
    "import pickle \n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pickle.load(open('../../data/cloth/cloth_all_infer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create structure of [[manifest per anchor]] \n",
    "pid = -1\n",
    "current_video = ''\n",
    "current_anchor = 0\n",
    "cloth_infer_group = []\n",
    "for idx, p in enumerate(result):\n",
    "    video = p[0]\n",
    "    anchor_id = p[1]\n",
    "    if video != current_video or anchor_id != current_anchor:\n",
    "        cloth_infer_group.append([])\n",
    "        current_video = video\n",
    "        current_anchor = anchor_id\n",
    "    cloth_infer_group[-1].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(cloth_infer_group, 200)\n",
    "result_vis = []\n",
    "for res in sample:\n",
    "    for p in res:\n",
    "        result_vis.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attri_name = ['pattern', 'major_color', 'necktie', 'collar', 'scarf', 'sleeve', 'neckline', 'clothing', 'jacket', 'hat', \\\n",
    "              'glass', 'layer', 'necktie_color', 'necktie_pattern', 'hair_color', 'hair_length']\n",
    "attri_dict = [\n",
    "        {'solid': 0, 'graphics' : 1, 'striped' : 2, 'floral' : 3, 'plaid' : 4, 'spotted' : 5}, # clothing_pattern\n",
    "        {'black' : 0, 'white' : 1, 'more color' : 2, 'blue' : 3, 'gray' : 4, 'red' : 5,\n",
    "                'pink' : 6, 'green' : 7, 'yellow' : 8, 'brown' : 9, 'purple' : 10, 'orange' : 11,\n",
    "                'cyan' : 12, 'dark blue' : 13}, # major_color\n",
    "        {'necktie no': 0, 'necktie yes' : 1}, # wearing_necktie\n",
    "        {'collar no': 0, 'collar yes' : 1}, # collar_presence\n",
    "        {'scarf no': 0, 'scarf yes' : 1}, # wearing_scarf\n",
    "        {'long sleeve' : 0, 'short sleeve' : 1, 'no sleeve' : 2}, # sleeve_length\n",
    "        {'round' : 0, 'folded' : 1, 'v-shape' : 2}, # neckline_shape\n",
    "        {'shirt' : 0, 'outerwear' : 1, 't-shirt' : 2, 'dress' : 3,\n",
    "            'tank top' : 4, 'suit' : 5, 'sweater' : 6}, # clothing_category\n",
    "        {'jacket no': 0, 'jacket yes' : 1}, # wearing_jacket\n",
    "        {'hat no': 0, 'hat yes' : 1}, # wearing_hat\n",
    "        {'glasses no': 0, 'glasses yes' : 1}, # wearing_glasses\n",
    "        {'one layer': 0, 'more layer' : 1}, # multiple_layers\n",
    "        {'black' : 0, 'white' : 1, 'more color' : 2, 'blue' : 3, 'gray' : 4, 'red' : 5,\n",
    "                'pink' : 6, 'green' : 7, 'yellow' : 8, 'brown' : 9, 'purple' : 10, 'orange' : 11,\n",
    "                'cyan' : 12, 'dark blue' : 13}, # necktie_color\n",
    "        {'solid' : 0, 'striped' : 1, 'spotted' : 2}, # necktie_pattern\n",
    "        {'black' : 0, 'white': 1, 'blond' : 2, 'brown' : 3, 'gray' : 4}, # hair_color\n",
    "        {'long' : 0, 'medium' : 1, 'short' : 2, 'bald' : 3} # hair_longth\n",
    "]\n",
    "attri_num = len(attri_name)\n",
    "attri_dict_inv = []\n",
    "for d in attri_dict:\n",
    "    d_inv = {}\n",
    "    for k, v in d.items():\n",
    "        d_inv[v] = k\n",
    "    attri_dict_inv.append(d_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import view_as_blocks\n",
    "\n",
    "def view_grid(im_in,ncols=3):\n",
    "    n,h,w,c = im_in.shape\n",
    "    dn = (-n)%ncols # trailing images\n",
    "    im_out = (np.empty((n+dn)*h*w*c,im_in.dtype)\n",
    "           .reshape(-1,w*ncols,c))\n",
    "    view=view_as_blocks(im_out,(h,w,c))\n",
    "    for k,im in enumerate( list(im_in) + dn*[0] ):\n",
    "        view[k//ncols,k%ncols,0] = im \n",
    "    return im_out\n",
    "\n",
    "GRID_W = 100\n",
    "GRID_H = 200\n",
    "img_data_dir = '../../data/cloth/cloth_all_img/'\n",
    "\n",
    "images = []\n",
    "cnt = 0\n",
    "for res in result_vis:\n",
    "    img = cv2.imread(os.path.join(img_data_dir, res[2]))\n",
    "    img_desp = np.zeros((GRID_H, GRID_W, 3))\n",
    "    row_height = int(GRID_H / (attri_num+2))\n",
    "    text = 'id: ' + str(cnt)\n",
    "    cnt += 1\n",
    "    cv2.putText(img_desp, text, (0, row_height), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3, color=(255,255,255), thickness=1)\n",
    "    for idx, c in enumerate(res[-1]):\n",
    "        text = attri_name[idx] + ' : ' + attri_dict_inv[idx][c]\n",
    "        cv2.putText(img_desp, text, (0, row_height*(idx+2)), cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.3, color=(255,255,255), thickness=1)\n",
    "    images.append(img)\n",
    "    images.append(img_desp)\n",
    "    \n",
    "images_np = np.zeros((len(images), GRID_H, GRID_W, 3))\n",
    "for i, im in enumerate(images):\n",
    "    if i % 2:\n",
    "        images_np[i] = im\n",
    "        continue\n",
    "        \n",
    "    H, W, C = im.shape\n",
    "    if 1. * H / W <= 2: \n",
    "        w = GRID_W\n",
    "        h = int(1. * H / W * w) \n",
    "    else:\n",
    "        h = GRID_H\n",
    "        w = int(1. * W / H * h)\n",
    "    im_regular = cv2.resize(im, (w, h))\n",
    "    images_np[i, :h, :w, :] = im_regular\n",
    "    \n",
    "grid = view_grid(images_np, 10)\n",
    "# H, W, C = grid.shape\n",
    "filename = '../data/cloth_test1.jpg'\n",
    "# grid_small = cv2.resize(grid, (WIDTH*ncol, int(H/W*WIDTH*ncol)))\n",
    "cv2.imwrite(filename, grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
